% This Source Code Form is subject to the terms of the Mozilla Public
% License, v. 2.0. If a copy of the MPL was not distributed with this
% file, You can obtain one at http://mozilla.org/MPL/2.0/.

@article{Duane1987,
abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
author = {Duane, Simon and Kennedy, A D and Pendleton, Brian J and Roweth, Duncan},
file = {:home/sree/1-s2.0-037026938791197X-main.pdf:pdf},
journal = {Physics Letters B},
number = {2},
pages = {216--222},
title = {{A.d. kennedy}},
url = {http://www.sciencedirect.com/science/article/pii/037026938791197X},
volume = {195},
year = {1987}
}

@phdthesis{Neal1993,
author = {Neal, Radford. M.},
booktitle = {Intelligence},
file = {:home/sree/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neal - 1993 - Probabilistic Inference Using Markov Chain Monte Carlo Methods.pdf:pdf},
keywords = {Gibbs sampler,Hamiltonian Monte Carlo},
number = {September},
school = {University of Toronto},
title = {{Probabilistic Inference Using Markov Chain Monte Carlo Methods}},
type = {Technical Report CRG-TR-93-1},
url = {http://omega.albany.edu:8008/neal.pdf http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.36.9055},
year = {1993}
}

@book{Neal1996,
abstract = {Artificial "neural networks" are widely used as flexible models for classification and regression applications, but questions remain about how the power of these models can be safely exploited when training data is limited. This book demonstrates how Bayesian methods allow complex neural network models to be used without fear of the "overfitting" that can occur with traditional training methods. Insight into the nature of these complex Bayesian models is provided by a theoretical investigation of the priors over functions that underlie them. A practical implementation of Bayesian neural network learning using Markov chain Monte Carlo methods is also described, and software for it is freely available over the Internet. Presupposing only basic knowledge of probability and statistics, this book should be of interest to researchers in statistics, engineering, and artificial intelligence.},
address = {Secaucus, NJ, USA},
author = {Neal, R. M.},
booktitle = {Journal of the Electrochemical Society},
file = {:home/sree/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neal - 1996 - Bayesian learning for neural networks.pdf:pdf},
isbn = {0387947248},
keywords = {Gibbs sampler,Hamiltonian Monte Carlo,markov chain monte carlo},
number = {July},
publisher = {Springer-Verlag New York, Inc.},
title = {{Bayesian learning for neural networks}},
url = {http://books.google.com/books?hl=en\&amp;lr=\&amp;id=\_peZjbrDC8cC\&amp;oi=fnd\&amp;pg=PA1\&amp;dq=Bayesian+Learning+for+Neural+Networks\&amp;ots=44y8XeZHDM\&amp;sig=OrPIAIQa22qeaSAgg4kfMpSAxYg http://portal.acm.org/citation.cfm?id=525544},
volume = {95},
year = {1996}
}

@article{Neal2012,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.1901v1},
author = {Neal, R},
eprint = {arXiv:1206.1901v1},
file = {:home/sree/Downloads/1206.1901v1.pdf:pdf},
journal = {Arxiv preprint arXiv:1206.1901},
title = {{MCMC using Hamiltonian dynamics}},
url = {http://adsabs.harvard.edu/abs/2012arXiv1206.1901N http://www.mcmchandbook.net/HandbookChapter5.pdf},
year = {2012}
}
